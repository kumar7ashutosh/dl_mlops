{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9e28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,tensorflow as tf\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88cb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c555c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/18 01:47:12 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run flawless-hare-359 at: https://dagshub.com/kumarashutoshbtech2023/dl_mlops.mlflow/#/experiments/1/runs/9222c9c479564c6bb8104dd2989f0996\n",
      "🧪 View experiment at: https://dagshub.com/kumarashutoshbtech2023/dl_mlops.mlflow/#/experiments/1\n",
      "✅ Logged successfully!\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/kumarashutoshbtech2023/dl_mlops.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"kumarashutoshbtech2023\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"c15604c69f0ae3b57c8400896b754673666d28a4\"\n",
    "\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(\"test-experiment\")  # will auto-create\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"lr\", 0.01)\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "\n",
    "print(\"✅ Logged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff66185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"artifacts/training/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493616b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    training_data: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc884900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cnn.constants import *\n",
    "from src.cnn.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e93c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=\"artifacts/training/model.h5\",\n",
    "            training_data=\"artifacts/data_ingestion/Chest-CT-Scan-data\",\n",
    "            mlflow_uri=\"https://dagshub.com/entbappy/chest-Disease-Classification-MLflow-DVC.mlflow\",\n",
    "            all_params=self.params,\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_batch_size=self.params.BATCH_SIZE\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7344762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0371ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def _valid_generator(self):\n",
    "\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale = 1./255,\n",
    "            validation_split=0.30\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    \n",
    "\n",
    "    def evaluation(self):\n",
    "        self.model = self.load_model(self.config.path_of_model)\n",
    "        self._valid_generator()\n",
    "        self.score = model.evaluate(self.valid_generator)\n",
    "        self.save_score()\n",
    "\n",
    "    def save_score(self):\n",
    "        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
    "        save_json(path=Path(\"scores.json\"), data=scores)\n",
    "\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        mlflow.set_experiment(\"CNN-Chest-CT-Scan\")\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Log all hyperparameters\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "\n",
    "            # Log metrics (ensure you set these earlier in your eval code)\n",
    "            mlflow.log_metrics({\n",
    "                \"accuracy\": self.model_accuracy,\n",
    "                \"loss\": self.model_loss\n",
    "            })\n",
    "\n",
    "            # Log model\n",
    "            mlflow.keras.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c92c94aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to endpoint /api/2.0/mlflow/runs/create failed with error code 404 != 200. Response body: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m mlflow.set_tracking_uri(os.environ[\u001b[33m\"\u001b[39m\u001b[33mMLFLOW_TRACKING_URI\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Directly start a run without relying on experiment lookup\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m metric_name, metric_value \u001b[38;5;129;01min\u001b[39;00m evaluation.metrics.items():\n\u001b[32m     10\u001b[39m         mlflow.log_metric(metric_name, metric_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:475\u001b[39m, in \u001b[36mstart_run\u001b[39m\u001b[34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001b[39m\n\u001b[32m    471\u001b[39m         user_specified_tags[MLFLOW_RUN_NAME] = run_name\n\u001b[32m    473\u001b[39m     resolved_tags = context_registry.resolve_tags(user_specified_tags)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     active_run_obj = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_id_for_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_system_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# If `log_system_metrics` is not specified, we will check environment variable.\u001b[39;00m\n\u001b[32m    483\u001b[39m     log_system_metrics = MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING.get()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\tracking\\client.py:479\u001b[39m, in \u001b[36mMlflowClient.create_run\u001b[39m\u001b[34m(self, experiment_id, start_time, tags, run_name)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_run\u001b[39m(\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    427\u001b[39m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     run_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    431\u001b[39m ) -> Run:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[33;03m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    477\u001b[39m \u001b[33;03m        status: RUNNING\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\telemetry\\track.py:24\u001b[39m, in \u001b[36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_telemetry_disabled() \u001b[38;5;129;01mor\u001b[39;00m _is_telemetry_disabled_for_event(event):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:183\u001b[39m, in \u001b[36mTrackingServiceClient.create_run\u001b[39m\u001b[34m(self, experiment_id, start_time, tags, run_name)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Extract user from tags\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# in a later release.\u001b[39;00m\n\u001b[32m    181\u001b[39m user_id = tags.get(MLFLOW_USER, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_current_time_millis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRunTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:338\u001b[39m, in \u001b[36mRestStore.create_run\u001b[39m\u001b[34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[39m\n\u001b[32m    328\u001b[39m tag_protos = [tag.to_proto() \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]\n\u001b[32m    329\u001b[39m req_body = message_to_json(\n\u001b[32m    330\u001b[39m     CreateRun(\n\u001b[32m    331\u001b[39m         experiment_id=\u001b[38;5;28mstr\u001b[39m(experiment_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     )\n\u001b[32m    337\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateRun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Run.from_proto(response_proto.run)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:201\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[39m\n\u001b[32m    199\u001b[39m     endpoint, method = \u001b[38;5;28mself\u001b[39m._METHOD_TO_INFO[api]\n\u001b[32m    200\u001b[39m response_proto = response_proto \u001b[38;5;129;01mor\u001b[39;00m api.Response()\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:571\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[39m\n\u001b[32m    568\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m    569\u001b[39m     response = http_request(**call_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m response_to_parse = response.text\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kumar\\Desktop\\anaco\\envs\\cnn\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:317\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m         base_msg = (\n\u001b[32m    314\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    315\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    318\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Response body: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    319\u001b[39m             error_code=get_error_code(response.status_code),\n\u001b[32m    320\u001b[39m         )\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint.startswith(_REST_API_PATH_PREFIX) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_parse_as_json_object(response.text):\n",
      "\u001b[31mMlflowException\u001b[39m: API request to endpoint /api/2.0/mlflow/runs/create failed with error code 404 != 200. Response body: ''"
     ]
    }
   ],
   "source": [
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "\n",
    "    # Directly start a run without relying on experiment lookup\n",
    "    with mlflow.start_run():\n",
    "    # Log metrics\n",
    "        for metric_name, metric_value in evaluation.metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    # Log parameters\n",
    "        for param_name, param_value in evaluation.params.items():\n",
    "            mlflow.log_param(param_name, param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(\"test-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(\"test-experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 123)\n",
    "    mlflow.log_metric(\"metric1\", 0.95)\n",
    "\n",
    "print(\"✅ Logged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
